\documentclass[fontsize=12pt]{article}
\usepackage[margin=0.75in]{geometry}

\usepackage[english]{babel}
\usepackage[activate=true,final,tracking=true]{microtype}

\usepackage[tt=false]{libertine}
\usepackage{libertinust1math, fontspec, amsmath, csquotes, hyperref, multicol, xcolor, minted, mathtools}
\usepackage[style=iso]{datetime2}

% \usepackage[style=apa, backend=biber, sortcites=true, sorting=nty]{biblatex}
% \usepackage[backend=biber, style=authoryear, bibstyle=authortitle, natbib=true, maxcitenames=2, maxbibnames=99, isbn=false, url=false, firstinits=true, uniquename=false, uniquelist=false, terseinits]{biblatex}
% \bibliographystyle{ama}
% \bibliography{refs.bib}
% \hypersetup{hidelinks}

\usepackage[style=apa, backend=biber, sortcites=true, sorting=nty]{biblatex}
\bibliography{refs.bib}
\hypersetup{hidelinks}


\setlength{\parindent}{1em} 
\setlength{\leftmargini}{1em}
\setlength{\leftmarginii}{0.75em}

% \definecolor{bg}{HTML}{343434}
\renewcommand\theFancyVerbLine{\small\arabic{FancyVerbLine}}
\definecolor{bg}{HTML}{EEEEEE}
\definecolor{white}{HTML}{FFFFFF}
\definecolor{blue}{HTML}{2c2c73}
\newcommand{\py}[1]{\mintinline[bgcolor=white, baselinestretch=1]{python}{#1}}

\setminted{style=friendly, bgcolor=bg, linenos, mathescape}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=blue,
    linkcolor=blue,
    urlcolor=blue
}
\setmonofont{Inconsolatazi4}
\linespread{1.33}

\title{Open Secrets: Investigating systemic biases in Wikipedia, the free online encyclopedia}
\author{Eamon Ma, Hisbaan Noorani, Rachel Xie, Philip Harker}
\date{\today}

\setlength{\columnsep}{16pt}
\begin{document}
\maketitle
\begin{multicols}{2}
    \section{Introduction}
Throughout CSC110 and CSC111, a great deal of discussion has centered around data. Data is defined as the aggregate sum of information in a given set, and the information we gain from data is the basis of our collective human knowledge. In the pre-computer days, information was filed away in encyclopedias and journals, in places where looking up and finding the information that one needed was time-consuming and often difficult. As computing technology became more powerful, though, innovators like Tim Berners-Lee had a vision of a system in which information could rapidly be retrieved. But, just as important, he envisioned a system in which all of this information was connected to each other. Any source, any related article could be pulled up with the press of a button. This was the real significance of the internet, and this vision shapes the way we build databases to this day.

Data is more present in our lives today than ever before. In aggregate, the sheer amount of data being stored on the internet by individuals is enormous. Poorly built databases can, even while preserving the accuracy and reliability of data, lose sight of Berners-Lee's vision of connectedness. Preserving data is one of the challenges of the modern data engineer. How do we ensure that our knowledge remains connected? How do we prevent critical information and thinking from becoming obscure and unlinked to the rest of the common knowledge of mankind?

Our objective as a team was to \textbf{use links between Wikipedia articles to determine areas in which our collective knowledge is lacking.} This was a very abstract goal that we developed as we
    
    \section{Description of dataset(s)}
    % name and description of ALL datasets; if you generate datasets, describe it; include format and source
    All of the data used in this project is open and available at \href{https://en.wikipedia.org/wiki/Wikipedia:Database\_download\#Where\_do\_I\_get\_it?}{https://en.wikipedia.org/wiki/Wikipedia:Database\_download\#Where\_do\_I\_get\_it?}. \parencite{WikimediaDownloads}
    % TODO: IS THIS THE RIGHT PAGE
    
    The sole source of data is the Wikipedia pages, available \href{https://meta.wikimedia.org/wiki/Data\_dump\_torrents\#English\_Wikipedia}{here}. \parencite{DataDumpTorrents}
    
    We used the most recent dataset available to us at the time we first began this project, from (2021-01-01).
    
    This dataset contains every single English Wikipedia page, accurate to 2021-01-01, compiled into one \textsc{xml} file. The singular \textsc{xml} datasets were downloaded via torrent due to its large size, to help reduce server load.
    
    \section{Computational overview}
    % note: only edit this if you're ready for eamon to murder you - RX
    % describe data we're using to represent our chosen domain; 
    \subsection{The data}
    The essence of the Web is captured in this dataset---pages identified with uniform resource locators, interconnected with hyperlinks within the pages. This structure naturally lends itself to the format of a graph: the very concept of a network \emph{is} a graph. Then, it is entirely instinctive to represent a website who is---by popular perception---a rabbit hole as a graph: its interconnected articles are the nodes of this graph, with edges---hyperlinks---connecting them. 
    On a more abstract note, knowledge is inherently connected in graph-like structures with varying intensity. Perhaps elementary algebra is a necessary prerequisite for the study of calculus, and perhaps the invention of ice cream is connected to the early life of Leonardo DiCaprio. No matter where we look, we can almost always find connections among pieces of knowledge.
    Therefore, it is sensible to represent the biggest open compilation of human knowledge as a graph. To limit complexity, we restrict our exploration to English Wikipedia. 

    \subsection{The processing}
    The overarching goal here is to build a graph of articles on Wikipedia, then perform analyses. The immensity of the dataset---over 81 gigabytes uncompressed---poses a challenge that would otherwise not be present with a smaller dataset. To perform useful analyses with reasonable computing times, we break the data transformation of the large \textsc{xml} file into small steps: \begin{enumerate}
        \item Create an index of line numbers of the boundary of the articles. We save this index after creation, as the generation process takes approximately thirty minutes on a modern computer.
        \item Partition the large \textsc{xml} into smaller subsets. We partitioned the file into 80 partitions, though the number of choice can be further fine-tuned. This has a number of advantages, notably the ability to parallelize the processing of the files. \begin{itemize}
            \item To partition the large file, we generate a list of partition points. This is achieved by splitting the file at roughly equal line numbers, at the boundary between pages.
        \end{itemize}
        \item Process the partitions. In conventional sequential processing, the advantage of partitions is shown in error resistance: if the processing fails at a given partition, the previous work is not lost. We can simply restart before the file which failed. In concurrent processing, we see the benefits manifest themselves in a stunning manner: the production of graph files from the entire eighty gigabytes takes only approximately 12 minutes. 
    \end{enumerate}
    \subsubsection{Indexing}
    Indexing serves to mark the dividing lines for subsequent partitioning. Essentially, the alternative is to iterate through the \textsc{xml} and match \texttt{<mediawiki>} tags at runtime, so it can be advantageous for this index to be computed in advance. Then, subsequent parties doing working on the data can decide themselves where to partition based on their system and needs.
    The function is simple, but can be a time and headache saver. Essentially, it looks for an opening page tag in each line; if such a tag is present, then it is saved as a boundary in \texttt{line\_numbers}. We make the assumption safely because the \textsc{xml} dump is computer-generated, thus consistent, well formed, and syntactically correct in this regard.

    \subsubsection{Partitioning}
    Partitioning serves to allow concurrency in the program, greatly speeding up practical running time of analysis functions on modern operating systems and \textsc{cpu}s which support concurrent processing. Before partitioning, we produce a list of numbers at which the file is split. The partitions are produced by iterating through the master \textsc{xml} file, incrementing a line counter, accumulating lines, and writing accumulated lines when a partition point is reached.

    \subsubsection{Processing}
    Processing the dataset---and by extension the partitions is run through a concurrent context manager, specifically \py{ProcessPoolExecutor}. This is a high-level library that enables the execution of asynchronous code with blocking operations \parencite{concurrent}. It differs from \py{ThreadPoolExecutor}, a similar library, in that \py{ProcessPoolExecutor} runs tasks with their own child process, while \py{ThreadPoolExecutor} runs tasks within threads in the main process \parencite{difference}. Specifically within the context managers, a process is submitted to a queue to be created for each individual partition. \py{process_partition} is called on each partition, and the results are saved to disk.
    For each partition, two \textsc{tab separated values} files, or \textsc{tsv}, are created. The first contains rows of articles in each partition and associated information such as whether or not it is a redirect, its character count, and the time delta between the last edit and 2021-01-01; the second contains rows of links each article contains. 
    Finally, we collapse the redirect articles. When visiting Wikipedia, occasionally there is small text below the article title indicating that one was \emph{Redirected from \_\_\_}. These are accomplished by articles whose sole purpose is to redirect, and so we must take these into account. 

    \subsubsection{Extracting information from wikitext}
    We saw experimentally that an available library takes on average about 30 seconds on a modern computer to parse through an \textsc{xml} file approximately one million lines long. (Note that though the parser is written for parsing wikitext, in contrast to \textsc{xml}, the mere \emph{presence} of markup does not affect the functionality.) Extrapolating, the 1.3 billion line Wikipedia dump would take about \(1300 \cdot 30\) seconds, or under 10 hours.
    
    Diving into the \href{https://github.com/5j9/wikitextparser/blob/master/wikitextparser/_wikilink.py}{code for \py{wikitextparser}}, we see the project relies on regular expressions as a primary method of string manipulation. Unfortunately, this tool falls short for large files, and performs functionality we do not need. Instead, we decided on a hybrid approach: match all the wikilinks using a regular expression, then conduct further processing on each wikilink with string operations. In combination with performing fewer computations, we see a significant improvement over the library: on that same million line file, the string operation implementation takes under 1.3 seconds over a 10 run average. This is more than an order of magnitude faster than the library's implementation---about 20-23 times faster, to be precise. This allows us to reduce the projected computing time for producing the graph from 10 hours to about half an hour.

    Subsequent information extracted from a given article's wikitext also use similar tricks of micro optimization. For instance, we notice that the length between the ending text element and the ending page element is constant. Then, we only need to find the beginning text tag. 

    \subsection{Score}
    We calculate a score for each article with the following equation, proceeding from defined attributes: \py{self.score} \\ \py{= (self.char_count * len(self.neighbours))} \\ \py{/ (math.log(self.last_edit) + 1)}

% \end{multicols}
\section{Obtaining the dataset and running the program}
% remember: TA assumes they can do everything we need them to do starting with a laptop with just Python 3.9 installed; describe what the TA should expect to see once they run main.py -- tell what data they'll learn, what they should see, include screenshots!
\begin{enumerate}
    \item Clone the project from Markus.
    \item Open the project root in the command line---paths mentioned hereafter are relative to project root. We highly recommend using a Python virtual environment: \begin{itemize}
        \item Create a venv: \py{python -m venv venv}
        \item On Windows cmd, run the bat script to activate the venv: \texttt{venv\textbackslash Scripts\textbackslash activate.bat}
        \item In shell, source the venv: \texttt{source venv/bin/activate}
    \end{itemize}
     Install requirements using pip3: \texttt{pip3 install -r requirements.txt}
    \item Install the project package with ``\py{pip3 install -e .}''
    \item Due to the impracticality of distributing a 17GB file on servers at the University of Toronto, if you wish to compute on the given original dataset, you may use your preferred BitTorrent client to obtain the file from \href{https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia}{Wikimedia's meta page}, ensuring that the file downloaded is \texttt{enwiki-20210101-pages-articles-multistream.xml.bz2 on nicdex.com (17.79 GB)}. \begin{itemize}
        \item Uncompress the original dataset to \texttt{data/raw/enwiki-20210101-pages-articles-multistream.xml}
    \end{itemize}
    \item If you wish to test the program on smaller datasets, download the provided \textsc{xml} files and place them accordingly: \begin{verbatim*}
data/raw/reduced/k.xml
data/raw/reduced/ninepointthreek.xml
data/raw/reduced/hundredk.xml
data/raw/reduced/million.xml
data/raw/reduced/animation.xml
data/raw/reduced/anarchism.txt
\end{verbatim*}
    Note that the first four files are produced with the shell command \texttt{head -n (\(m\) x) > \(\text{word}(m)\).xml}, where x is the original file, where m in [ 1000, 10000, 100000, 1000000 ], and \(\text{word}(m)\) is a function that maps m to the respective item in [ k, ninepointthreek, hundredk, million ]. Subsequently, \texttt{echo "</page>" >> \(\text{word}(m)\).xml} and \texttt{echo "</mediawiki>" >> \(m\).xml}. The latter two files are produced by manually extracting an \textsc{xml} page, and extracting wikitext, respectively.
    \item If you wish to produce graphs for these truncated datasets, manually obtain the precise line count with \texttt{wc -l \(\text{word}(m)\).xml} in shell, or the appropriate tool on Windows. Then, edit the constant \py{FILE_LINE_COUNT} at the top of \texttt{wikigraph/partition\_data.py} to the number obtained.
\end{enumerate}

For best results, we strongly recommend at least 16GB of \textsc{ram} with adequate memory paging schemes set up for your operating system. 

% \begin{multicols}{2}
    \section{Justification and discussion of changes}
    % changes between proposal and final submission; based on discussions, new ideas, TA feedback
    The provided teaching assistant feedback was considered carefully. A few of the provided suggestions had already been addressed in the initial project proposal, such as the suggestion to download and process the data once, storing it for future use to avoid having to load from this enormous dataset every time. It was also suggested that we reduce the size of the dataset we were working with to avoid having to work with the enormous dataset representing every single one of Wikipedia's articles; however, it is necessary to leave the dataset as it is because there was no clear way to separate out just a smaller group of articles---we would have had to find some group of articles that link only to each other and never to any articles outside of that group, and doing that manually would be incredibly time-consuming, if not impossible. Indeed, this defeats the purpose of our research question.
    
    The major change from our proposal was our choice to change the metrics we were considering to find ``under-connected" articles in the dataset. To get view counts, we would have had to download another 3.5 TB of data, which is far too much for the relatively limited applicability of this metric. Looking at how much a page is viewed might tell us how often people are searching up certain topics, but that would not necessarily provide interesting results specific to Wikipedia. We also decided to consider character counts instead of word counts because this was simpler to get from the raw data and could be done more efficiently. This would, overall, increase the base numbers we get for this metric, but higher character counts generally also mean higher word counts, so the end result (finding the shortest articles on Wikipedia) is the same.
    
    We also stated that we would be using the Python library \texttt{NetworkX}, but did not use it in the end as \texttt{NetworkX} was quite slow and our dataset was quite large---prioritizing efficiency was a necessity. We neglected to use \texttt{pandas} as our method for processing the data never led to us needing it. The new library we chose to visualize data with was \texttt{Pyvis}. While it is not exceptionally fast, it does provide a loading screen and somewhat more visually appealing interactive windows for the user to look at. We would have liked to find a faster library for visualization, but finding one for Python was extremely difficult.
    
    \section{Discussion}
    % discuss, analyze, and interpret program results; include some stuff about potential further exploration
    There is necessarily error in our results because Wikipedia by its very nature is open source. Even with its guidelines and standards, there will be syntax errors and inconsistencies between articles. There are mistakes caused by editor error that our program simply cannot account for. However, we believe the dataset is large enough that the overall results are still worth analyzing. From the first one million lines of the data, there are 4279 articles. Within those, we found 16 incorrect links as a result of Wikipedia editor errors--a 0.3\% error rate.
    
    % OUR CODE IS IMPECCABLE; PEOPLE ARE JUST DUMB.
    
    The results of our program show...???
    
    For future research, we might extend the functionality of the program to sort the articles it finds that score the lowest on a given metric by topic---i.e., when considering the articles with the lowest character counts, it might be interesting to then group those articles based on what they're about. % again, is this true? are we going to be doing this??
    
    % todo: add a concluding statement

\end{multicols}

\linespread{2}
\newpage
\nocite{*}
\printbibliography[title={\centering References}]

\end{document}
