\documentclass[fontsize=12pt]{article}
% \usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry}

\usepackage[activate=true,final,tracking=true]{microtype}

\usepackage[tt=false]{libertine}
\usepackage{libertinust1math, fontspec, amsmath}

\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[style=apa, backend=biber, sortcites=true, sorting=nty]{biblatex}
\bibliography{refs.bib}
\usepackage{hyperref}
\hypersetup{hidelinks}
\usepackage[style=iso]{datetime2}

\linespread{1.33}

\title{Open Secrets: Investigating systemic biases in Wikipedia, the free online encyclopedia}
\author{Eamon Ma, Hisbaan Noorani, Rachel Xie, Philip Harker}
\date{March 16, 2021}

\usepackage{multicol}
\setlength{\columnsep}{16pt}
\begin{document}
\maketitle
\begin{multicols}{2}
    % Some Noteworthy Reads:
    % https://ijoc.org/index.php/ijoc/article/viewFile/777/631
    % https://www.theatlantic.com/technology/archive/2011/05/is-wikipedia-a-world-cultural-repository/239274/
    % https://iccl.inf.tu-dresden.de/web/Wikidata/Maps-06-2015/en
    % https://diff.wikimedia.org/2015/08/07/systemic-bias-wikipedia-vs-publishers/

    % very specifically, our topic is about articles on wikipedia that have Less Information than they should -- we'll ignore the issue of people being able to edit whatever they want -- FOCUS ON SYSTEMIC BIASES

    \section{Problem description and research question}  % this is currently almost exactly 300 words

    As a free online encyclopedia run by volunteer collaborators, edits to Wikipedia articles form the backbone of the website.
    Without its open nature, Wikipedia would not be what it is today.
    However, this does not guarantee greatness --- for the same reasons it is a symbol of communal knowledge and free information, Wikipedia is inherently biased.
    Most notably, it is biased towards the countries that articles are edited most in, and the dominant cultures in those countries.
    ``[Our] research does show that most editors to Wikipedia come from the United States and Western Europe. And, as of 2020, our survey data indicate that fewer than 1\% of Wikipedia’s editor base in the U.S. identify as Black or African American. Considering these data, we can say with certainty that we are missing important perspectives from the world that Wikipedia strives to serve.'' \parencite{Uzzell}.
    Along with an abundance of information of interest to Western society, there is a distinct lack of information from and for marginalized groups.

    The coverage of knowledge is not spread evenly across all the Wikipedia articles.
    It varies based on what is desired and what is available, and often, the most available information (on English Wikipedia\footnote{For the purposes of this project, only English Wikipedia will be considered -- demographics and articles of interest will vary between languages, so we will focus on only English articles to avoid confusion.}) concerns men from ``developed'' countries and their interests.
    Only 17.82\% of Wikipedia's biographies are about women.
    This isn't merely an issue of Wikipedia's editors specifically being unreliable, it is an issue of there not being reliable sources in the wider Internet available for editors to feel confident creating articles about topics like women in science \parencite{Erhart}.
    In this way, Wikipedia can be thought of as a representation of society's general knowledge base.

    Our research question is, \textbf{``How can we use connections / links between Wikipedia articles to determine areas where our collective knowledge is lacking?}

    % more specific examples of areas where there is clear discrepancy between academic knowledge and what's on Wikipedia, and also i guess patterns of stub articles -- is it more frequently female historical figures that have less information on them compared to male historical figures? just as an example

    % we can also do some preliminary testing for ourselves on articles of interest and figure out some specific articles where this kind of things happens (by targeting those articles specifically) - HN
    % if it helps at all, i know there's missing information in the page about the Babylonian astronomical diaries -- u of t's library system has a ton of info on them - RX

    \section{Computational plan}

    % \setcounter{subsection}{-1}
    % \subsection{About the dataset}
    The data to be used is an offline copy of all of Wikipedia's articles.
    There are a few reasons that this dataset is offline instead of live.
    \begin{enumerate}
        \item Wikipedia request that research not be done on live versions of articles as it puts unnecessary strain on their servers --- we would like to respect their wishes.
        \item Changes to the dataset may occur midway through computation.
        This would likely lead to errors in graph generation if references to articles are edited while the computation is taking place.
        \item Computational time.
        Working with an offline dataset is much faster than working with an online one.
        If the online version were to be used, the user would have to wait for the download of each article, every time that a re-calculation is run, instead of a single time at the beginning and then never again.
        % If we were to use the online version, we would have to wait for it to download each article every time that a re-calculation is run, instead of a single time at the beginning and then never again.
        \item There will be significantly less bandwidth used with an offline dataset.
        The download will only need to be performed once, at the very beginning, and not every time the user wishes to recompute the graph.
        This leads to approximately 18 GB of data savings compressed, and 78 GB of data savings uncompressed.
    \end{enumerate}
    Additionally, there will be reduced computational complexity with the offline dataset as fewer operations will need to be performed.

    Our computational plan consists of three main stages.
    \textit{Stage 1, Stage 2,} and \textit{Stage 3}.
    For now, it will be assumed that every single article on English Wikipedia will be graphed, however this may change later on as the scope of the project is shifted due to computational and time constraints.

    \subsection{Stage 1: Processing the data}
    A graph of links between Wikipedia articles will be created.
    % We plan to create a graph of links between Wikipedia articles.
    These links are the hyperlinks visible on an article (typically in blue text) that link to other articles.
    External links will be ignored.
    A little bit more information about each article such as the word count, number of viewers, number of editors, number of edits, etc. will be stored.
    % We also wish to store a little bit more information about each article such as the word count, number of viewers, number of editors, number of edits, etc.
    This means that a simple \texttt{Graph} and \texttt{\_Node} structure will not suffice.
    A new \texttt{Graph}, \texttt{\_Node} pair with instance attributes to represent the extra data that is to be collected will be created.
    The exact data to be collect is subject to change as the viability of collecting each metric and the use that it will provide are considered.

    Due to the large size of the dataset, processing the data is a time-consuming endeavour.
    This means that it should be avoided as much as possible.
    A possible solution to this issue is to save the graph in plain-text so that a re-computation is not required every time the graph is loaded into the visualization system.

    A challenge arises when considering how to store such a graph.
    A format will be created that saves this graph in parts.
    First: as a set of nodes in the graph, second: a list of the edges in the graph, and third: a file that will save all other information for each node in the graph.
    This file may be formatted in a manner similar to or matching a JSON or CSV file.

    \subsection{Stage 2: Analysis}
    After the graph is created, nodes with characteristics determined to be found in underrepresented articles will be singled out programmatically.
    After this, each article will be analyzed on both an individual and a macroscopic level.
    We will look for trends in the articles that we find to be underrepresented for reasons other than age of article, obscurity of topic (low demand), and other metrics.
    The trends discovered in this analysis will allow us to find more such articles that may not display the same severity of symptoms, and also allow us to look at the state of the internet as a whole.

    \subsection{Stage 3: Visualization}
    After this preliminary analysis is completed, a visualization system will be developed that will allow the user to find underrepresented articles, view them in relation to other articles around them, and visit the actual articles. The python library \texttt{networkx} will likely be used to create this visualization and in order to interface with \texttt{networkx}, the python library \texttt{pandas} will also be used for its \texttt{dataframe} objects.
    This will allow the user to discover why these articles are underrepresented in a visual manner, which many people find will likely find easier to understand than a written report.

    % either by themselves, or we can write a blurb for particular articles that stand out to us as being underrepresented for an actual reason. We could also identify these articles by the trends that we find in the analysis section. - HN

    % collect statistics about things like average number of edges for an article; perhaps make some graphs of those stats; could talk about this more in stage 3 - RX
    % I feel like this part will probably go into analysis instead of visualization as the visualization part is literally just creating a thing so that people can easily look at our results... at least, that's what I envisioned it as - HN

    \section{Datasets}

    All of the data that will be used from this project is open and available at \href{https://dumps.wikimedia.org/}{https://dumps.wikimedia.org/}. \parencite{WikimediaDownloads}
    The first of the two main sources of data will be the Wikipedia pages, available \href{https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia}{here}. \parencite{DataDumpTorrents}
    The most recent dataset at the time of writing is the one that will be used (2021-01-01) .
    This dataset is simply every single English Wikipedia page accurate to 2021-01-01, compiled into one XML file. A smaller, sample dataset is available \href{http://itorrents.org/torrent/B23A2BDC351E58E041D79F335A3CF872DEBAE919.torrent}{here}. All of the page datasets must be downloaded via torrent due to their large size, to help reduce server load. Due to the nature of this dataset, it is not possible to provide an example.

    The second main source of data will be analytical data on the uses of Wikipedia pages, such as page view counts. This is available \href{https://dumps.wikimedia.org/other/analytics/}{here}. The following is an example of some of the data found in the dataset:

    \texttt{en.wikipedia 12\_Songs\_(album) desktop 1 J1}

    This can be better represented as a table:
    \end{multicols}

    \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    subproject.project & Article & Page id & Daily Total & Hourly Counts \\
    \hline
    en.wikipedia & 12\_Songs\_(album) & desktop & 1 & J1 \\
    \hline
    \end{tabular}
    \end{center}

    \(\text{ }\)

    \begin{multicols}{2}
    \subsection*{Explanation of Data Table}
    \begin{itemize}
        \item \texttt{subproject.project}

        This is the domain that the article is hosted on.

        \item \texttt{Article}

        This is the title of the article.

        \item \texttt{Page id}

        This distinguishes between the platform of access.

        \item \texttt{Daily Total}

        This is the total number of requests that the page receives for a given day.
        This is not the same thing as unique visitors.

        \item \texttt{Hourly Counts}

        This is a representation of when the clicks happened in the day. It can be deciphered as follows. The hours of the day go from 0 to 23.
        In this encoding scheme, each letter represents its direct count in hours.
        This means that \(A\) represents 0, \(B\) represents 1, \dots, \(W\) represents 22, \(X\) represents 23.
        The number following the letter is the count of requests in that hour.
        This means that the extracted data row says that there was 1 request for the page \texttt{12\_Songs\_(album)} at the 9\(^{\text{th}}\) hour of the day on the first of December, 2011 (from the file name, each file represents one day).
    \end{itemize}

    % \underline{Foreseeable issues}
    % \begin{itemize}
    %     \item VERY LARGE DATASET
    %     \item storing a graph because we don't want to generate that again
    % \end{itemize}
    %
    % \underline{Plan or something}
    % \begin{itemize}
    %     \item Create a graph of connections from one wiki article to another
    %     \begin{itemize}
    %         \item Use the Wikipedia python library to retrieve pages or get a dataset
    %         \item All the hyperlinks
    %     \end{itemize}
    %     \item Compute the shortest connections
    %     \item a graph. computational challenge: how to save / generate graphs
    %     \begin{itemize}
    %         \item Make like charts and stuff
    %         \item For fun, compute the shortest length between any given two things with an interface of some sort
    %     \end{itemize}
    % \end{itemize}

    % \underline{BRAINSTORMING FOR PROBLEM DOMAIN}
    % \begin{itemize}
    %     \item problem domain; research: we know for a fact that there are gaps in Wikipedia's knowledge -- we can get both the links between articles and the articles in Wikipedia to find which pages have fewer links (and maybe lower word counts) may indicate gaps in our knowledge; we could examine the articles with fewest connections for patterns
    %     \item more specific area to look at: [and also how would we solve this computationally]
    %     \item for funsies can find shortest path between two unrelated objects (could also pivot from initial idea and turn this into a “how to best play the Wikipedia game or whatever it's called”)
    % \end{itemize}
    %
    % \underline{BRAINSTORMING FOR THINGS THAT WE WILL DO}
    % \begin{itemize}
    %     \item consider connections between articles in a certain area of knowledge (north American history, as a completely wild example) to see what most articles are connected to and what places are not as likely to be found just by hopping through Wikipedia links
    %     \item see if our program can find gaps in available knowledge (esp. since we saw on international women’s day Wikipedia itself was talking about a deficit of information on female historical figures -- we know for a fact there's gaps in the available information) based on comparing number of connections between pages as well as word counts of pages  -- maybe we'll even find patterns in what's missing
    % \end{itemize}

\end{multicols}

% References, currently shows all citations in refs.bib (\nocite{*})

\linespread{2}
\newpage
% \nocite{*}
\printbibliography[title={\centering References}]

\end{document}
