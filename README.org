#+TITLE: Bruh

# Just a little todo list so that we can work on things at times outside of the times when we meet up. Assign yourself things here and then we can work on our own tasks, and mark them off from the main list once they're done.

# - [ ] This is an uncompleted task
# - [-] This is a task that is in progress
# - [X] This is a completed task

** Dev instructions (from project root)
1. Create a venv: ~python -m venv venv~ and enter the venv:
  + ~source venv/bin/activate~ on Unix
  + ~venv\Scripts\activate.bat~ on Windows
2. Install requirements: ~pip install -r requirements.txt~
3. Install local package: ~pip install -e .~
4. Place data files in respective locations in ~/data/~
5. Run tests: ~pytest -v~

  -----
  -----

* To-do list
** Project Proposal [1/1]
- [X] Do it
** Dataset processing [1/7]
- [X] Add ~Graph~ and ~_Vertex~ implementations
- [ ] Figure out which database we're going to use and how to use it (SQL?)
- + [ ] Create a python implementation for reading from the database +
- [ ] Figure out a file format that we will use for storing processes graphs
- [ ] Create a load_graph() function that will load a graph from that stored format
- [ ] Get the word count in each page and the respective talk page. Add this into the format above (as well as the ~Graph~ and ~_Vertex~ implementations)
- [ ] Actually do the processing (Test on a smaller dataset first)
** Visualization [0/3]
- [-] Figure out another library to use (~networkx~ is slow for large graphs)
- [ ] Write the implementation
- [ ] Allow for information about the page on hover (maybe using the ~wikipedia~ library with some caching?)
** Analytics [0/1]
- [-] Develop/find and program an algorithm to search for the vertices with the least edges (and other criteria that we look for which will be decided upon later)
- [ ] Maybe look at pages that we know to be underrepresented and look at the trends in them, then search for those in other pages.
** Project Report [0/8]
- [-] Introduction
- [-] Computational Overview (Methods)
- [ ] Instructions
- [ ] Analysis
- [ ] Discussion
- [ ] Conclusion
- [ ] References (Just add as we go)
- [ ] Justify changes

-----
-----

* Group Members
** Eamon [x/x]
- [X] Implement wikilink parser
- [X] Implement wikitext count char
- [X] Implement wikitext get last revision
- [X] Implement experiment to test practical running time
- [X] Implement experiment to test practical difference between collect_links_wikitext and wtp
- [X] Implement tests for above
** Hisbaan [2/2]
- [X] Do it
- [X] Add ~Graph~ and ~_Vertex~ implementations
** Philip [1/1]
- [X] Do it
- [-] Report introduction
- [-] Scouting new visualization library 
** Rachel [1/?]
- [X] Develop/find and program algorithm(s) to search for the vertices with the least edges
- [-] Consider other criteria for analysing the graph

-----
-----

* Broad strokes overview of computational plan
- Split the dataset up into multiple datasets. We ensure that the division does not split up one page into two datasets.
- Parse each part of this dataset in parallel. For each page, we extract...
  + All of the edges that it possesses.
  + The word count of the article.
  + The last edit (if this is accurate).
  + anything else that we can pull from the xml.
- Merge the restuls from the parallel operations.
- Use this extracted information to create a graph object -- a loader method like the one that we did for graphs and weighted graphs in A3.
- Find some articles that we know are lacking in content/research.
- Find common traits of these articles. Also include custom characteristics that we are sure are present in lacking articles.
- Use some sort of graph searching algorithm to find all the other articles that match these characteristics.
- Visualize this graph:
  + Use some sort of clustering algorithm. Can we do anything with this? Will a node that is in a cluster be, on average, more fleshed out?
  + Show the graphs that are not well represented as another colour.
  + Maybe show statistics like the title, word count, degree, etc on hover.
- We're done!

-----
-----

* File Structure: IMPORTANT

Each of these subpoints will be a directory in the repo. Try to ensure that your code is as cleaned up as possible when you are pushing and that you are not pushing unnecessary files or you don't have files in the wrong location.

The root directory will contain things like this README, requirements.txt, etc. Try not to clutter it up too much with things that would do better placed in a subdirecotry.

** data

This directory is meant for data storage. This will not be pushed, but the structure will remain. We don't push this because it's bad practice to push file that are obtainable outside of the project (especially if these files are large)

*** raw

Raw files that have not yet been processed. This inlucdes the wikidump.

**** reduced

Smaller sections of the wikidump that we can run trials on.

*** processed

This is where output will go. We may push some of these or find some other way to share these as the processing time will be insane.

** proposal

Directory for the project proposal. Only push tex, pdf, and bib files.

** report

Directory for the project report. Only push tex, pdf, and bib files.

** src

This is where all the python files will go. There should generally be no subfolders here but there are some exceptions. This is to allow for proper PATH management (how python modules are imported, etc).

All python files here will need to include the following

#+begin_src python
"""Module docstring"""
import os  # Toward the top of the file

if __name__ == '__main__':
    os.chdir(__file__[0:-len('name of file')])
#+end_src

This code ensures that the code runs relative to the src directory, no matter where you execute it from. This smooths out some differences between vscode and pycharm/terminal python. I know that some of our TAs use vscode so this is NECESSARY.

We should also make sure to document our code very well.

** test

This directory is where we will put unit tests but it is also okay to have random testing for other things. Try to make sure that your code is as clean as possible when you're pushing things.

-----
-----

* Notes
** Creating Graph
- Initialize all the vertices, then all the edges because it's not organized in an orderly way (like the reviews thing where one dataset could only link to a member of the other)
** Finding Links
*** Initial impresssions
- Everything inside of ~[[]]~ is a link.
- Anything after a ~|~, we can ignore.
- Some issues with brackets (e.g. ~kingdom (biology)~ redirects to Biological Kingdom, ~Wikipedia:Style~)
- don't use wikitextparser library because that's where most of the complexity from the project comes from so we should probably do it ourselves
- Don't use regex --- it's slow as shit
*** How to do
- Look for a double open brace (~[[~)
- If a page contains ~<redirect title = "Something Here" />~, then we can label it a redirect with an instance attribute when we add the vertex to the graph and then, we will just redirect to the page that it wants to be redirected to when it wants to be
  + If a page is a redirect, then we don't collect information about it
** Saving graph
- Save edges something like
  #+begin_src python
dictionary = {
    vertex1: {edges1},
    vertex2: {edges2},
    vertex2: {edges3}
}
  #+end_src
- Save the information about each vertex something like
  #+begin_src csv
vertex1,redirects_to,charcount1,otherthings1
vertex2,redirects_to,charcount2,otherthings2
vertex3,redirects_to,charcount3,otherthings3
  #+end_src
- Save all the information in ram first, then write to file after all the processing is done. This will be significantly faster
- This redirects_to will be an empty column if it is not a redirect and it will contain the name of the vertex that it redirects to if it redirects to a vertex. In the second case (it is a redirect) the other columns will be empty or 0 or whatever
** Metrics
- Number of edges (links to page, and pages that it links to? Maybe only one)
- Char count
- Delta between the first of january 2021 and the timestamp (last edit)
- Number of citations (count ~{{cite~)
** Visualization
*** Possible Libraries
- Pygraphviz requires a C / C++ compiler
- Zen is allegedly a faster thing than Networkx but its website is nonfunctional
- graph-tool is faster than Networkx (multiple sources claim this) but requires either installing docker or otherwise doing weird non-Pycharm stuff that our TAs might not be willing to do
- snap.py allegedly claims to be good for analysing big networks but the tutorial says the visualization functionality should only be used for small graphs --- it uses Graphviz to do this
- PyVis can directly be installed in PyCharm, allows creation of interactive graphs, may not actually be faster than Networkx though

  from some comments online, "if the graph is too big Pyvis will re-create the graph after altering the data, and for that it has to load it all over again (which could take some time). I think there is no work-around over this particular problem, as it is in the esence of the package"
  + Just a for whoever wrote this, you don't install something in PyCharm. PyCharm uses the pip package manager to install it. Anything that is pycharm specific is a no-no for us. We don't know what ide our TAs are going to be using so we don't want to do anything that is locked down. PyVis works as it's not pycharm specific but just be weary of that.

** Justifying changes
- Getting rid of the view counts because 3.5 TB of data is too much, and also, it's not really that helpful --- it doesn't really matter for "connection of knowledge."
- Why we can't do small dataset:
  well the thing is, if we split it, it wouldn't be an issue
  I think that 1000 is way too small to do anything meaningfull, because articles will link to other articles right? (those are the edges) That limits our stuff a lot... and we can't sort based on obscurity, because that's exactly what we're trying to show exists right?

  like maybe we go only biographies right? But then some guys is a mathematician... oh no... now he's linking to all the stuff that he invented

  oh we should include that so we can do things like look at paths... oh no... someone was an english literature person. now we include that stuff and pretty soon, we have all of wikipedia

** Wikilink parser known issues
- None at the moment
